{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3908323",
   "metadata": {},
   "source": [
    "üõ†Ô∏è What is a Tool in LangChain?\n",
    "üîπ A Tool is a callable function or utility that the LLM can invoke to perform tasks that are outside its own knowledge or capabilities.\n",
    "Think of it like an external plugin or helper function that gives the LLM superpowers, such as:\n",
    "\n",
    "Searching Wikipedia\n",
    "\n",
    "Fetching academic papers\n",
    "\n",
    "Retrieving private documents (using a vectorstore)\n",
    "\n",
    "Calling an API\n",
    "\n",
    "\n",
    "üß† Analogy\n",
    "üß† LLM = Human brain\n",
    "üß∞ Tool = Calculator, Library, Phone, Web browser\n",
    "\n",
    "If a human is answering a question and doesn't know the answer, they:\n",
    "\n",
    "Ask someone (Wikipedia tool)\n",
    "\n",
    "Look it up in their notes (retriever tool)\n",
    "\n",
    "Search Google Scholar (Arxiv tool)\n",
    "\n",
    "The same applies here. LLMs can \"use tools\" like humans do.\n",
    "\n",
    "‚öôÔ∏è What Does a Tool Contain?\n",
    "In LangChain, a Tool is a Python object that includes:\n",
    "\n",
    "\n",
    "| Component          | Description                                                            |\n",
    "| ------------------ | ---------------------------------------------------------------------- |\n",
    "| `name`             | A short string name used by the LLM to call the tool                   |\n",
    "| `description`      | Tells the LLM **when to use** this tool                                |\n",
    "| `func` or `invoke` | The actual Python function to execute when the tool is used            |\n",
    "| `args_schema`      | (Optional) A schema for validating and auto-generating input arguments |\n",
    "\n",
    "\n",
    "\n",
    "üîΩ Under the Hood: How a Tool Works\n",
    "Let's say you define this tool:\n",
    "\n",
    "\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def search_docs(query: str) -> str:\n",
    "    return \"Some search results for: \" + query\n",
    "\n",
    "my_tool = Tool(\n",
    "    name=\"doc_search\",\n",
    "    description=\"Useful for answering questions about internal docs.\",\n",
    "    func=search_docs\n",
    ")\n",
    "\n",
    "LangChain under the hood wraps this into a JSON-like function call schema\n",
    "\n",
    "\n",
    "‚úÖ So the LLM sees:\n",
    "\"When the user asks about internal documentation, I can call a function doc_search(query) with a string input.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "üîÅ Flow of Tool Usage\n",
    "Here‚Äôs what happens step-by-step in a tool call:\n",
    "\n",
    "üîπ Step 1: User Input\n",
    "User asks something like:\n",
    "\n",
    "Tell me about LangSmith‚Äôs observability features.\n",
    "\n",
    "üîπ Step 2: LLM Analyzes Input\n",
    "The LLM realizes it needs external help, based on the tool descriptions.\n",
    "\n",
    "\n",
    "üîπ Step 3: Tool Call Generation\n",
    "LLM generates a structured call like this:\n",
    "\n",
    "{\n",
    "  \"tool_call\": {\n",
    "    \"name\": \"langsmith-search\",\n",
    "    \"arguments\": {\n",
    "      \"query\": \"LangSmith‚Äôs observability features\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "üîπ Step 4: LangChain Executes the Tool\n",
    "LangChain takes the tool name and args\n",
    "\n",
    "It calls the associated Python function:\n",
    "retriever.invoke(\"LangSmith‚Äôs observability features\")\n",
    "\n",
    "\n",
    "üîπ Step 5: Result is Returned to LLM\n",
    "Tool returns a string like:\n",
    "LangSmith provides real-time tracing, error analysis, and observability dashboards...\n",
    "\n",
    "üîπ Step 6: LLM Composes Final Answer\n",
    "LLM reads the result and generates a human-friendly answer:\n",
    "\n",
    "\"LangSmith offers rich observability including real-time tracing and dashboards that help developers understand LLM behavior...\"\n",
    "\n",
    "\n",
    "\n",
    "      +------------------------+\n",
    "      |   User Asks Question   |\n",
    "      | \"What is LangSmith?\"   |\n",
    "      +-----------+------------+\n",
    "                  |\n",
    "                  v\n",
    "        +---------+----------+\n",
    "        |      LLM (Gemma)   |\n",
    "        |  + Prompt & Tools  |\n",
    "        +---------+----------+\n",
    "                  |\n",
    "          Decides tool is needed\n",
    "                  |\n",
    "                  v\n",
    "   +--------------+-------------------+\n",
    "   |     Tool Call by LLM             |\n",
    "   | name = \"langsmith-search\"        |\n",
    "   | args = { \"query\": \"LangSmith\" }  |\n",
    "   +--------------+-------------------+\n",
    "                  |\n",
    "                  v\n",
    "      +-----------+-----------+\n",
    "      |  LangChain Tool Runs  |\n",
    "      |  retriever.invoke()   |\n",
    "      +-----------+-----------+\n",
    "                  |\n",
    "       Gets relevant documents\n",
    "                  |\n",
    "                  v\n",
    "       +----------+----------+\n",
    "       |  Tool returns result |\n",
    "       | \"LangSmith helps...\" |\n",
    "       +----------+----------+\n",
    "                  |\n",
    "        LLM uses result to respond\n",
    "                  |\n",
    "                  v\n",
    "     +------------+------------+\n",
    "     |  Final Answer to User   |\n",
    "     |  \"LangSmith provides...\"|\n",
    "     +-------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af07262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LANGCHAIN_PROJECTS\\Q&A_Chat_bot\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ebcae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d47c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020303685820>, search_kwargs={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs=loader.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "vectordb=FAISS.from_documents(documents,embeddings)\n",
    "retriever=vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb8c898",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# creating a custom tool\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretriever\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_retriever_tool\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m retriever_tool = create_retriever_tool(\u001b[43mretriever\u001b[49m,\u001b[33m\"\u001b[39m\u001b[33mlangsmith-search\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mSearch any information about Langsmith\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m retriever_tool.name\n",
      "\u001b[31mNameError\u001b[39m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# creating a custom tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(retriever,\"langsmith-search\",\"Search any information about Langsmith\")\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c58db69",
   "metadata": {},
   "source": [
    "‚úÖ What it does:\n",
    "Takes your FAISS retriever (built using LangSmith docs) and wraps it as a tool.\n",
    "\n",
    "The name is what the LLM will refer to when calling the tool.\n",
    "\n",
    "The description is used by the LLM to decide when to use this tool during a conversation.\n",
    "\n",
    "üìå Now the LLM knows:\n",
    "\n",
    "\"If the user is asking about LangSmith, I should consider using the langsmith-search tool.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffb5ea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='langsmith-search', description='Search any information about Langsmith', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000203050C72E0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020303685820>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000203050C76A0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020303685820>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c13ac629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(groq_api_key = groq_api_key,model_name=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e79955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e9142",
   "metadata": {},
   "source": [
    "üìú Load a Tool Calling Prompt\n",
    "‚úÖ What it does:\n",
    "Pulls a pre-defined chat prompt from the LangChain Hub.\n",
    "\n",
    "\"hwchase17/openai-functions-agent\" is a standard function-calling agent prompt.\n",
    "\n",
    "It includes:\n",
    "\n",
    "System message describing tool usage.\n",
    "\n",
    "Placeholders like {tools} and {tool_names}.\n",
    "\n",
    "Instructions to guide the LLM to use tools when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53b1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arxiv--Research\n",
    "## Tools creation\n",
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper,ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43a5ded6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Used the inbuilt tool of wikipedia\n",
    "api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fc2573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_arxiv=ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "arxiv=ArxivQueryRun(api_wrapper=api_wrapper_arxiv)\n",
    "print(arxiv.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381fbebd",
   "metadata": {},
   "source": [
    "üß± The Two-Layer Design\n",
    "When using tools like Wikipedia or Arxiv in LangChain, you are working with a two-layer architecture:\n",
    "| Layer                 | Class                                     | Role                                                      |\n",
    "| --------------------- | ----------------------------------------- | --------------------------------------------------------- |\n",
    "| **1. Low-level**      | `WikipediaAPIWrapper` / `ArxivAPIWrapper` | Wraps the raw API logic (fetching, filtering, formatting) |\n",
    "| **2. Tool Interface** | `WikipediaQueryRun` / `ArxivQueryRun`     | Converts the wrapper into a usable LangChain `Tool`       |\n",
    "\n",
    "1Ô∏è‚É£ WikipediaAPIWrapper / ArxivAPIWrapper\n",
    "‚úÖ Purpose:\n",
    "This is the low-level utility that:\n",
    "\n",
    "Knows how to query a source (Wikipedia or Arxiv)\n",
    "\n",
    "Parses and cleans up the data\n",
    "\n",
    "Limits or filters the content (e.g., top-k, max chars)\n",
    "\n",
    "‚úÖ What it does:\n",
    "Formats your query for Wikipedia or Arxiv\n",
    "\n",
    "Sends the request (usually via requests or httpx)\n",
    "\n",
    "Parses the result (e.g., HTML ‚Üí plain text)\n",
    "\n",
    "Trims it based on parameters like:\n",
    "\n",
    "top_k_results: How many results to return\n",
    "\n",
    "doc_content_chars_max: Max characters in content\n",
    "\n",
    "‚úÖ Why it's called a \"Wrapper\":\n",
    "Because it wraps the actual API ‚Äî hiding the raw HTTP calls and providing a clean, reusable Python function.\n",
    "\n",
    "wiki_wrapper = WikipediaAPIWrapper(top_k_results=1)\n",
    "wiki_wrapper.run(\"LangChain\")\n",
    "# ‚Üí Returns a string summary of LangChain from Wikipedia\n",
    "\n",
    "\n",
    "2Ô∏è‚É£ WikipediaQueryRun / ArxivQueryRun\n",
    "‚úÖ Purpose:\n",
    "This wraps the wrapper into a LangChain Tool interface so it can be used by agents.\n",
    "\n",
    "Think of it as the Tool adapter that turns your wrapper into a fully-functioning LangChain Tool.\n",
    "\n",
    "‚úÖ What it does:\n",
    "Takes the WikipediaAPIWrapper or ArxivAPIWrapper\n",
    "\n",
    "Exposes a .invoke() method compatible with LangChain Tools\n",
    "\n",
    "Auto-generates input/output schema (function calling)\n",
    "\n",
    "Adds a default name, description, and callable\n",
    "\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=wiki_wrapper)\n",
    "wiki_tool.invoke(\"LangChain\")\n",
    "# Internally calls: wiki_wrapper.run(\"LangChain\")\n",
    "# Returns the Wikipedia content in a Tool-compatible way\n",
    "\n",
    "\n",
    "üí° Why Not Just Use the Wrapper?\n",
    "Good question!\n",
    "\n",
    "You can use the wrapper directly in your own Python code, but for LangChain Agents, tools must follow a specific interface:\n",
    "\n",
    "Have a name\n",
    "\n",
    "Have a description\n",
    "\n",
    "Be callable via .invoke() with a single input\n",
    "\n",
    "Return a string (or ToolOutput)\n",
    "\n",
    "That's exactly what QueryRun classes are for ‚Äî they standardize the wrapper into a Tool.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "üß† Analogy\n",
    "Think of Wrapper as a \"Car Engine\"\n",
    "And QueryRun as the \"Steering Wheel and Dashboard\"\n",
    "\n",
    "You need both to actually drive the car (use the tool in a LangChain agent).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db6b5807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith-search'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(retriever,\"langsmith-search\",\"Search any information about Langsmith \")\n",
    "\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fdc85a",
   "metadata": {},
   "source": [
    "1. üì• Input: retriever\n",
    "This is any object that implements BaseRetriever, like:\n",
    "\n",
    "Chroma.as_retriever()\n",
    "\n",
    "This retriever must implement the method:\n",
    "\n",
    "retriever.invoke(query: str) -> List[Document]\n",
    "\n",
    "Which returns the top-k relevant documents for a query.\n",
    "\n",
    "\n",
    "2. üß† Output: a Tool object\n",
    "The Tool object looks like this under the hood:\n",
    "\n",
    "Tool(\n",
    "    name=\"vector_search\",\n",
    "    description=\"Search for relevant information...\",\n",
    "    func=retriever_function,\n",
    "    coroutine=None,  # optional for async\n",
    "    args_schema=None # auto-generated from retriever\n",
    ")\n",
    "So now the retriever is callable as a tool, with name/description that the LLM can use to call it properly in agent settings.\n",
    "\n",
    "3. ‚öôÔ∏è Internal Logic\n",
    "Here‚Äôs what create_retriever_tool actually does:\n",
    "\n",
    "  -->Wraps the retriever in a Tool with a callable function retriever.invoke.\n",
    " \n",
    "  -->Auto-generates schema so that the agent knows the input (usually just a    query: str).\n",
    "\n",
    "  -->Provides a name and description so the LLM knows when to use this tool.\n",
    "\n",
    "This lets you use it like:\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[retriever_tool],\n",
    "    llm=ChatGroq(),\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "üß† Bonus: Behind the Scenes\n",
    "Here's a simplified version of how LangChain defines it:\n",
    "\n",
    "def create_retriever_tool(retriever, name: str, description: str) -> Tool:\n",
    "    def retriever_func(query: str) -> str:\n",
    "        docs = retriever.invoke(query)\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    return Tool(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        func=retriever_func\n",
    "    )\n",
    "\n",
    "So essentially:\n",
    "\n",
    "It takes a query ‚Üí runs it through the retriever ‚Üí returns the combined document content as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "633293ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki,arxiv,retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6644f",
   "metadata": {},
   "source": [
    "‚úÖ What it does:\n",
    "Collects all three tools:\n",
    "\n",
    "Wikipedia (general purpose)\n",
    "\n",
    "Arxiv (research papers)\n",
    "\n",
    "LangSmith FAISS Retriever (custom docs)\n",
    "\n",
    "This list will be passed into the agent so it knows what tools it can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f3e42da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000203526C8AE0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000203526C8AE0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000203072DFE00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002030731BB30>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith-search', 'description': 'Search any information about Langsmith ', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "agent = create_openai_tools_agent(llm,tools,prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6151230b",
   "metadata": {},
   "source": [
    "‚úÖ What it does:\n",
    "Creates an LLM-powered agent using OpenAI-style tool calling schema.\n",
    "\n",
    "Behind the scenes, it sets up:\n",
    "\n",
    "The ChatPromptTemplate with instructions and tool names.\n",
    "\n",
    "Logic for interpreting LLM tool calls and routing them.\n",
    "\n",
    "Wrapping the tools into function-callable specs.\n",
    "\n",
    "Now the agent can:\n",
    "\n",
    "Read your prompt.\n",
    "\n",
    "Decide to call one of the tools.\n",
    "\n",
    "Combine the results into a coherent answer.\n",
    "\n",
    "This uses the OpenAI-compatible function-calling spec, which Groq‚Äôs model can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cafae64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000203526C8AE0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000203526C8AE0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000203072DFE00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002030731BB30>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith-search', 'description': 'Search any information about Langsmith ', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'd:\\\\LANGCHAIN_PROJECTS\\\\Q&A_Chat_bot\\\\venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)), Tool(name='langsmith-search', description='Search any information about Langsmith ', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000203050C72E0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020303685820>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000203050C76A0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020303685820>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agent Executer\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe6368",
   "metadata": {},
   "source": [
    "‚úÖ What it does:\n",
    "Wraps the agent into an executor, which handles:\n",
    "\n",
    "Repeated calls\n",
    "\n",
    "Input formatting\n",
    "\n",
    "Tracing (if enabled)\n",
    "\n",
    "Tool usage display (verbose mode)\n",
    "\n",
    "verbose=True prints step-by-step what the agent is doing, including:\n",
    "\n",
    "What tool it picks\n",
    "\n",
    "What query it sends\n",
    "\n",
    "What result it gets\n",
    "\n",
    "Final response from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b612e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith-search` with `{'query': 'Langsmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
      "\n",
      "LangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî¬†it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\n",
      "For more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "Observability‚Äã\n",
      "Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\n",
      "This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\n",
      "\n",
      "Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\n",
      "\n",
      "Get started by adding tracing to your application.\n",
      "Create dashboards to view key metrics like RPS, error rates and costs.\n",
      "\n",
      "Evals‚Äã\n",
      "The quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\n",
      "\n",
      "Get started by creating your first evaluation.\n",
      "Quickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\n",
      "Analyze results of evaluations in the LangSmith UI and compare results over time.\n",
      "Easily collect human feedback on your data to improve your application.\n",
      "\n",
      "Prompt Engineering‚Äã\n",
      "While traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Langsmith',\n",
       " 'output': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. \\n'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Tell me about Langsmith\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67c10cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'machine learning'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Machine learning\n",
      "Summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks wit\u001b[0m\u001b[32;1m\u001b[1;3mMachine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without being explicitly programmed.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is machine learning',\n",
       " 'output': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without being explicitly programmed.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"What is machine learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2eaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6787fda",
   "metadata": {},
   "source": [
    "so how many times an llm call is invoked as we can see that llm has to invoke to decide a tool then that tool give some output this output is sent to the llm and then the llm takes that output and invokes once again and give the final output is it like that\n",
    "\n",
    "\n",
    "\n",
    "üîÅ Number of LLM Calls in a Tool-Based Agent Flow\n",
    "‚úÖ In a typical AgentExecutor (like in LangChain ReAct or OpenAI function agents), the LLM is called multiple times per user question:\n",
    "üîÑ Step-by-Step Breakdown (ReAct or Function-Calling Agent):\n",
    "üß© Step 1: First LLM Call ‚Äî Decide on Action\n",
    "LLM is called with the user query and tool descriptions.\n",
    "\n",
    "It returns something like this:\n",
    "Thought: I need to look up information about LangSmith.\n",
    "Action: langsmith-search\n",
    "Action Input: LangSmith observability features\n",
    "‚úîÔ∏è LLM Call #1 happens here to decide which tool to use.\n",
    "\n",
    "üîß Step 2: LangChain Executes Tool\n",
    "LangChain reads:\n",
    "\n",
    "Action = langsmith-search\n",
    "\n",
    "Action Input = LangSmith observability features\n",
    "\n",
    "So it calls:\n",
    "retriever.invoke(\"LangSmith observability features\")\n",
    "This gives some text output (e.g., retrieved docs or API result).\n",
    "\n",
    "üß† No LLM call here ‚Äî only the tool function runs.\n",
    "\n",
    "üß† Step 3: Second LLM Call ‚Äî Final Answer\n",
    "LangChain then calls the LLM again, passing:\n",
    "\n",
    "The original question\n",
    "\n",
    "The tool output (observation)\n",
    "\n",
    "And asks:\n",
    "\n",
    "\"Now, using this observation, answer the user's question.\"\n",
    "Observation: LangSmith provides tracing features to monitor...\n",
    "Thought: I now understand how to answer.\n",
    "Final Answer: LangSmith‚Äôs observability helps developers...\n",
    "\n",
    "[User Question] \n",
    "      |\n",
    "      v\n",
    "[LLM Call #1 ‚Üí Decides tool]\n",
    "      |\n",
    "      v\n",
    "[LangChain runs tool]\n",
    "      |\n",
    "      v\n",
    "[LLM Call #2 ‚Üí Observes & decides next step]\n",
    "      |\n",
    "      ‚îú‚îÄ‚îÄ (Done?) ‚Üí [Final Answer] ‚úÖ\n",
    "      ‚îî‚îÄ‚îÄ (Need another tool?) ‚Üí [Repeat]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f981cd8",
   "metadata": {},
   "source": [
    "‚úÖ Two Main Ways to Use LangChain Agents\n",
    "1. initialize_agent(...)\n",
    "\n",
    "search_agent = initialize_agent(...)\n",
    "response = search_agent.run(input)\n",
    "\n",
    "2. AgentExecutor(...)\n",
    "agent_executor = AgentExecutor(agent=..., tools=..., verbose=True)\n",
    "response = agent_executor.invoke({\"input\": \"...\"})\n",
    "\n",
    "\n",
    "initialize_agent(...) already returns a fully wrapped AgentExecutor by default.\n",
    "\n",
    "\n",
    "initialize_agent(...) ‚áí returns an instance of AgentExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cbabec",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
